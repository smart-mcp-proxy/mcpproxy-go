# Followup: OAuth Token Refresh Bug Fixes

**Date**: 2026-01-17
**Branch**: `023-oauth-state-persistence-impl`
**Status**: Fixed, pending end-to-end verification

## Problem Statement

After implementing spec 023 (OAuth State Persistence), OAuth servers with expired tokens were failing to refresh automatically despite having valid `refresh_token` values stored in the database. Servers would show errors like:

```
MCP initialization failed after OAuth setup: no valid token available, authorization required
```

This affected dynamically-discovered OAuth servers (e.g., atlassian-remote, slack, github) that use Protected Resource Metadata (PRM) for OAuth discovery rather than static OAuth configuration.

## Investigation Summary

### Diagnostic Approach

Added comprehensive logging to trace the token refresh flow:

1. **RefreshManager startup** (`internal/oauth/refresh_manager.go`)
   - Logs token loading from storage
   - Logs each token's expiration status and refresh_token availability
   - Logs schedule creation count

2. **PersistentTokenStore** (`internal/oauth/persistent_token_store.go`)
   - Logs GetToken/SaveToken calls with server_key
   - Logs token expiration and refresh_token status

3. **Server key generation** (`internal/oauth/persistent_token_store.go`)
   - Logs key generation with server_name, server_url, and generated_key

4. **OnTokenSaved callback** (`internal/runtime/lifecycle.go`)
   - Logs when tokens are saved and refresh is scheduled

### Diagnostic Findings

From the logs, we verified:
- RefreshManager IS starting correctly (9 schedules created)
- OnTokenSaved callback IS firing
- Tokens ARE being saved with correct server_key format

But `RefreshOAuthToken` was failing to find tokens for dynamically-discovered OAuth servers.

## Bugs Found and Fixed

### Bug #1: Static Config Check Only

**Location**: `internal/upstream/manager.go:2362-2379`

**Problem**: `RefreshOAuthToken` only checked `serverConfig.OAuth` to determine if a server uses OAuth. For dynamically-discovered OAuth servers, this field is `nil` because OAuth was discovered at runtime via PRM, not configured statically.

**Fix**: Also check the database for stored OAuth tokens:

```go
// Check if server uses OAuth via either static config or dynamic discovery
serverConfig := client.GetConfig()
hasStaticOAuth := serverConfig != nil && serverConfig.OAuth != nil

// Also check for OAuth tokens in the database (dynamic OAuth discovery)
hasStoredTokens := false
if m.storage != nil && serverConfig != nil {
    serverKey := oauth.GenerateServerKey(serverName, serverConfig.URL)
    token, err := m.storage.GetOAuthToken(serverKey)
    if err == nil && token != nil && token.RefreshToken != "" {
        hasStoredTokens = true
    }
}

if !hasStaticOAuth && !hasStoredTokens {
    return fmt.Errorf("server does not use OAuth: %s", serverName)
}
```

### Bug #2: Token Lookup Key Mismatch (Critical)

**Location**: `internal/upstream/manager.go:2371`

**Problem**: The initial fix for Bug #1 used `serverName` directly for token lookup:

```go
token, err := m.storage.GetOAuthToken(serverName)  // WRONG
```

But tokens are stored using a hash-based key generated by `GenerateServerKey(name, url)`:

```go
// Tokens are stored with key like "slack_a1b2c3d4e5f6g7h8"
serverKey := GenerateServerKey(serverName, serverURL)
```

This mismatch meant `RefreshOAuthToken` could never find tokens for any server, causing all refresh attempts to fail.

**Fix**: Use `GenerateServerKey` for consistent key lookup:

```go
serverKey := oauth.GenerateServerKey(serverName, serverConfig.URL)
token, err := m.storage.GetOAuthToken(serverKey)  // CORRECT
```

## Files Modified

| File | Changes |
|------|---------|
| `internal/upstream/manager.go` | Fixed OAuth detection and token lookup key |
| `internal/upstream/manager_oauth_test.go` | Added test coverage for dynamic OAuth servers |
| `internal/oauth/refresh_manager.go` | Added diagnostic logging |
| `internal/oauth/persistent_token_store.go` | Added server key generation logging, enhanced GetToken logging |
| `internal/runtime/lifecycle.go` | Enhanced OnTokenSaved callback logging |

## Test Coverage

New test file: `internal/upstream/manager_oauth_test.go`

- `TestRefreshOAuthToken_DynamicOAuthDiscovery` - Tests servers with OAuth from PRM
- `TestRefreshOAuthToken_StaticOAuthConfig` - Tests servers with static OAuth config
- `TestRefreshOAuthToken_ServerNotFound` - Tests error handling

All tests passing.

## Verification Steps

To verify the fix works end-to-end:

1. **Rebuild**:
   ```bash
   go build -o mcpproxy ./cmd/mcpproxy
   ```

2. **Start with debug logging**:
   ```bash
   ./mcpproxy serve --log-level=debug
   ```

3. **Authenticate an OAuth server** (e.g., slack)

4. **Restart mcpproxy**

5. **Verify auto-reconnect** - Server should reconnect using stored token without browser re-authentication

6. **Check logs for**:
   ```
   Found OAuth token in database for dynamic OAuth server
   OAuth token refresh requested  has_stored_tokens=true
   ```

## Root Cause Analysis

The `GenerateServerKey` function creates a unique storage key by hashing `serverName|serverURL`:

```go
func GenerateServerKey(serverName, serverURL string) string {
    combined := fmt.Sprintf("%s|%s", serverName, serverURL)
    hash := sha256.Sum256([]byte(combined))
    hashStr := hex.EncodeToString(hash[:])
    return fmt.Sprintf("%s_%s", serverName, hashStr[:16])
}
```

This design allows multiple servers with the same name but different URLs to have separate tokens. However, any code that looks up tokens must use this same key generation, not just the server name.

The bug was introduced because the original `RefreshOAuthToken` implementation predated the `PersistentTokenStore` and didn't account for the hash-based storage key.

## Related Documentation

- Spec: `/specs/023-oauth-state-persistence/spec.md`
- Research: `/specs/023-oauth-state-persistence/research.md`
- Bug doc: `/docs/oauth_mcpproxy_bug.md`
- Plan: `/Users/josh.nichols/.claude/plans/magical-soaring-corbato.md`
